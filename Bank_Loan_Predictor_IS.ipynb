{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bank_Loan_Predictor_IS",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1bYdarVOsKT7AZToBgtdPcCUjfvyQyMIB",
      "authorship_tag": "ABX9TyPEb+Rb1Bgn5VwPRipEe1mF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashwith25/Marketing-Campaign-for-Banking-Products/blob/master/Bank_Loan_Predictor_IS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEllk-qL5uDR",
        "colab_type": "text"
      },
      "source": [
        "# Marketing Campaign for Banking Products\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLH1YK7TLwqG",
        "colab_type": "text"
      },
      "source": [
        "The bank has a growing customer base. The bank wants to increase borrowers (asset\n",
        "customers) base to bring in more loan business and earn more through the interest on\n",
        "loans. So , the bank wants to convert the liability based customers to personal loan\n",
        "customers. (while retaining them as depositors). A campaign that the bank ran last year\n",
        "for liability customers showed a healthy conversion rate of over 9% success. The\n",
        "department wants you to build a model that will help them identify the potential\n",
        "customers who have a higher probability of purchasing the loan. This will increase the\n",
        "success ratio while at the same time reduce the cost of the campaign."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhPMsV7YNzxd",
        "colab_type": "text"
      },
      "source": [
        "Attribute Information:\n",
        "- ID: Customer ID\n",
        "- Age: Customer's age in completed years\n",
        "- Experience: #years of professional experience\n",
        "- Income: Annual income of the customer ($000)\n",
        "\n",
        "- ZIP Code: Home Address ZIP code.\n",
        "- Family: Family size of the customer\n",
        "- CCAvg: Avg. spending on credit cards per month ($000)\n",
        "\n",
        "- Education: Education Level. 1: Undergrad; 2: Graduate; 3:\n",
        "Advanced/Professional\n",
        "- Mortgage: Value of house mortgage if any. ($000)\n",
        "- Personal Loan: Did this customer accept the personal loan offered in the last\n",
        "campaign?\n",
        "- Securities Account: Does the customer have a securities account with the bank?\n",
        "- CD Account: Does the customer have a certificate of deposit (CD) account with\n",
        "the bank?\n",
        "- Online: Does the customer use internet banking facilities?\n",
        "- Credit card: Does the customer use a credit card issued by the bank?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpXzV95nOTt3",
        "colab_type": "text"
      },
      "source": [
        "<strong>Objective:<br>\n",
        "The classification goal is to predict the likelihood of a liability customer buying personal\n",
        "loans.</strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inaDHhzeL5FA",
        "colab_type": "text"
      },
      "source": [
        "<b> To deal with this problem, we will split the process into 9 steps for better understanding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT_pGr8OMDh4",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "## 1. Import the datasets and libraries, check datatype, statistical summary, shape, null values etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4_3_GjLMWFv",
        "colab_type": "text"
      },
      "source": [
        "We will be importing all the necessary libraries which will be required in our project.\n",
        "- numpy, scipy and pandas for data analysis.\n",
        "- matplotlib and seaborn for plotting graphs\n",
        "- sklearn for training our model\n",
        "- pickle for saving our best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO4eLsQB5vpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plot\n",
        "import seaborn as sns\n",
        "import io\n",
        "import pickle\n",
        "from sklearn.preprocessing import PowerTransformer, MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from google.colab import files\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEvbJaZtMtEC",
        "colab_type": "text"
      },
      "source": [
        "Now we will be importing out CSV file which have the necessary data. Let's have a quick look on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2xiy98JxcoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fl = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KabAhBFO6JCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# content = pd.read_csv(\"/content/drive/My Drive/internship_studio_project.csv\")\n",
        "content = pd.read_csv(io.BytesIO(fl[list(fl.keys())[0]]))\n",
        "content.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raVdWiGJMqwJ",
        "colab_type": "text"
      },
      "source": [
        "Now let's see if our data is completely/properly uploaded or not. This can be done either by looking at the shape or by looking at the end of the dataset.\n",
        "Let's first see the end of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CnQL8pj6pdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRKFhOSNPQwv",
        "colab_type": "text"
      },
      "source": [
        "To check the shape of the dataset, <b>shape</b> variable is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0MlbeArAcM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N2hGkA4NZVK",
        "colab_type": "text"
      },
      "source": [
        "It is necessary to initially check if all the data is proper or not. This we can do using <b>describe()</b>. This returns us all the necessary data such as mean, standard deviation, min, max, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OHZ-ugD7IqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONQn5jnkOI_S",
        "colab_type": "text"
      },
      "source": [
        "As we can see in the _Experience_ column, the <b>min</b> value is -3 which is not possible. This indicates that the cleaning of the data must be done. \n",
        "\n",
        "In a similar way as that of <b>describe()</b>, we can also see the datatypes and non-null value count by using an in-built function called <b>info()</b>. This will return the count of non-null values along with the datatype of thaat particular column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2_6ldrI7Mam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNwxxSqCPeCh",
        "colab_type": "text"
      },
      "source": [
        "This completes our 1st step. All the necessary libraries are imported. We saw what all are the attributes of our Columns, based upon which some decisons were also taken where cleaning of the data is to be done as the _Experience_ column had some wrong data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbuVEEt6Ra1_",
        "colab_type": "text"
      },
      "source": [
        "Now let's move onto our 2nd step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PkDhQSXRf_x",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"2\"></a>\n",
        "## 2. Check if you need to clean the data for any of the variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9VIuYdLRjdh",
        "colab_type": "text"
      },
      "source": [
        "Firstly, let's first remove all the data which had faulty _Experience_.</br>\n",
        "This we will be storing in a new variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6PeGqBA1A37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_3cPRBCAalO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_content = content.where(content[\"Experience\"]>=0).dropna()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6ASyzCBSBHF",
        "colab_type": "text"
      },
      "source": [
        "Now let's see if our data is ready to be used or not by using <b>describe()</b> as used earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R92p-ZrZJ4W6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_content.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BVcpofT0OH8",
        "colab_type": "text"
      },
      "source": [
        "Let's save the target feature in some other variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD3Ss7Np7Pof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "personal_loan = new_content[\"Personal Loan\"]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLAx6DeSTb2j",
        "colab_type": "text"
      },
      "source": [
        "As _ID_ will be different for each customer, it has nothing to with the customer taking the loan. So it is acting as noise in our dataset. Hence it will be better if removed. <br><br>\n",
        "To check the significance of a column, we can check their correlation factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRQ9bKuH-qfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in new_content.columns:\n",
        "    print(f\"Correlation between {i} and Personal Loan is\",new_content[i].corr(personal_loan))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KAPEoLwUgB5",
        "colab_type": "text"
      },
      "source": [
        "As we can see, _ID, Age, ZIP Code_ and _Experience_ are hardly even related to our target variable. _Income_ plays a important role in customer taking the personal loan. So we can drop the noisy columns (_ID, Age_ and _Experience_) to make our prediction for efficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yf8adJKVZvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_content.drop([\"ID\", \"Age\", \"ZIP Code\", \"Experience\"], axis=1, inplace=True)\n",
        "new_content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "653V0ySsyYwF",
        "colab_type": "text"
      },
      "source": [
        "This completes are 2nd step. Till now we have imported the necessary libraries and cleaned our data.<br><br>\n",
        "Now let's move on to step 3 where we'll be studying our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJrKVHHEsqEg",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"3\"></a>\n",
        "## 3. EDA: Study the data distribution in each attribute and target variable, share your findings.<a name=\"abcd\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yam_AND6reh5",
        "colab_type": "text"
      },
      "source": [
        "Initially, we need to check what are the unique elements in each columns. This is possible using <b>unique()</b>, this returns a list of all the unique elements of that column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqB8eN2KyvFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in new_content.columns:\n",
        "    print(f\"Unique elements of {i} are {new_content[i].nunique()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ef93SUAtDoD",
        "colab_type": "text"
      },
      "source": [
        "People with zero mortgage value are: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRhn_IX5_4o4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"People with zero mortgage value are\",len(new_content.where(new_content[\"Mortgage\"]==0).dropna()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJSwJO2HtSTP",
        "colab_type": "text"
      },
      "source": [
        "Number of people with zero credit card spending per month are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jVNhJSGDUiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"People with zero credit card spending are\",len(new_content.where(new_content[\"CCAvg\"]==0).dropna()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJFZ_fVRtZ8F",
        "colab_type": "text"
      },
      "source": [
        "Value counts of all categorical columns are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpQiLnUwGzv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "education = new_content[\"Education\"].value_counts()\n",
        "family = new_content[\"Family\"].value_counts()\n",
        "features = [\"Securities Account\", \"CD Account\", \"Online\", \"CreditCard\"]\n",
        "yes_no = {0:\"No\", 1:\"Yes\"}\n",
        "degree = {1: \"Undergraduate\", 2: \"Graduate\", 3:\"Advanced/Professional\"}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qATAMJ_2HVOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Details of Customers opting for,\")\n",
        "for i in features:\n",
        "    print(\"\\n\",i)\n",
        "    y_n = new_content[i].value_counts()\n",
        "    for j in y_n.index:\n",
        "        print(f\"{yes_no[j]}:\\t {y_n.loc[j]}\")\n",
        "print(\"\\nEducation levels of Customers are,\")\n",
        "for i in education.index:\n",
        "    print(f\"{degree[i]}: {education.loc[i]}\")\n",
        "print(\"\\nFamily strength of customers are,\")\n",
        "for i in family.index:\n",
        "    print(f\"{int(i)}: {family.loc[i]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnBRiJtPhFkk",
        "colab_type": "text"
      },
      "source": [
        "Plotting each feature can give us an idea on whether transformations are needed or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrvCHVetJni7",
        "colab_type": "text"
      },
      "source": [
        "Correlation matrix would also help us identify the relationships between each and every features which we can compare with the above pairplot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZwXuYPNbBI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plot.subplots(figsize = (15,10))\n",
        "sns.heatmap(new_content.corr(), cmap=\"Greys\", annot = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pd8KaTdjbhb",
        "colab_type": "text"
      },
      "source": [
        "<b>Univariate Analysis are graphs plotted considering only one variable.</b>\n",
        "</br>\n",
        "Univariate analysis of all the columnbs/features bare given below.\n",
        "<a name=\"boxplot\"></a>\n",
        "As, only _Income, CCAvg and Mortgage_ are the only fields who are highly ccorrelated with the target variable and the ones who might possibly have outliers. <a name=\"outlier\"></a>Outliers are values which are either extremely high or extremely low as compared to values of InterQuartile Range (IQR). <br>\n",
        "This can be verified byt plotting boxplots for each of these features using seaborn's <b>boxplot()</b> method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rIx5rfqTBM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in [\"Income\", \"CCAvg\", \"Mortgage\"]:\n",
        "    sns.boxplot(new_content[i])\n",
        "    print(\"Skew value: \", new_content[i].skew())\n",
        "    plot.title(i)\n",
        "    plot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiQNCM5wwPyd",
        "colab_type": "text"
      },
      "source": [
        "The points marked with black dots represents <b>Outliers</b>. It is necessary to improve/remove them for better results. This we will be doing it in step 4.\n",
        "<a name=\"countplot\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgIKk788fpiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(new_content[\"Personal Loan\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcty-7tEf1AE",
        "colab_type": "text"
      },
      "source": [
        "We can see that very few people have opted for the Personal Loan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FtnlWKptRoW",
        "colab_type": "text"
      },
      "source": [
        "Let's see what relationships do our feature variables have with our target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mcH7w32gJAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.scatterplot(new_content.Income, new_content[\"Personal Loan\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deYQULQ8iaC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.barplot(new_content.Income, new_content[\"Personal Loan\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bowXskLbs8X2",
        "colab_type": "text"
      },
      "source": [
        "By observing above two graphs, we can conclude that with increase in income, there are high chances that a customer would opt for Personal Loan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAXVULQzxboX",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"pairplot\"></a>\n",
        "Let's now plot each feature against every other feature to get an idea about their relationship. This can be done using seaborn's <b>pairplot()</b>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roSaL_Qcj9-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.pairplot(new_content, hue=\"Personal Loan\", markers=[\"+\",\"D\"])\n",
        "plot.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsLlU9jiIJd3",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"distplot\"></a>\n",
        "It is necessary to have our data as normalised as possible to increase the accuracy. Plotting them can help us identify them. For this, <b>distplot()</b> would be a good option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BauQ_zdG1xBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in [\"Income\", \"CCAvg\", \"Mortgage\"]:\n",
        "    sns.distplot(new_content[i])\n",
        "    plot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wGD4Q8GIltW",
        "colab_type": "text"
      },
      "source": [
        "Our datas are skewed to the right. Infact _Mortgage_ is highly skewed. Hence, we need to apply the necessary transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9bIMnapK43m",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"4\"></a>\n",
        "## 4. Applying necessary transformations for the feature variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sp8rJ3sK_mS",
        "colab_type": "text"
      },
      "source": [
        "As we saw above, our data had a lot of outliers. Let's remove them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw4kwNPGMZFL",
        "colab_type": "text"
      },
      "source": [
        "Outliers can be detected using multiple ways. Here we are making use of InterQuartile Range(IQR) to detect and remove the outliers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8uTPagfMr1z",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"IQR\"></a>\n",
        "IQR is the range between the 75% of data(Q3) and 25% of data(Q1). \n",
        "We define outliers as any value higher than Q3 + (1.5 * IQR). Similar is the case with lower bound. Any value less than Q1 - (1.5 * IQR) is treated as outlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfkiksXdPqIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lower = new_content[\"Income\"].quantile(0.25)\n",
        "upper = new_content[\"Income\"].quantile(0.75)\n",
        "IQR = upper-lower\n",
        "upper_bound = upper + 1.5*IQR\n",
        "lower_bound = lower - 1.5*IQR\n",
        "print(lower_bound, upper_bound)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYzQtDBrYMIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lower1 = new_content[\"CCAvg\"].quantile(0.25)\n",
        "upper1 = new_content[\"CCAvg\"].quantile(0.75)\n",
        "IQR1 = upper1-lower1\n",
        "upper_bound1 = upper1 + 1.5*IQR1\n",
        "lower_bound1 = lower1 - 1.5*IQR1\n",
        "print(lower_bound1, upper_bound1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q7jIsjj-Oo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.boxplot(new_content.Income)\n",
        "plot.show()\n",
        "sns.boxplot(new_content.CCAvg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rByVubeqdZEN",
        "colab_type": "text"
      },
      "source": [
        "As we observed, that the outliers are beyond the upper bound for _Income and CCAvg_, we need to discard them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehkTdcyWWdX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = new_content[new_content[\"Income\"]<upper_bound]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1_AJtqKYZB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[df[\"CCAvg\"]<upper_bound1]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oneY41Y5_Rx-",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"outlier-removed\"></a>\n",
        "Now that we have removed the outliers, we can check if the outliers are reduced or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUTGhy9N-8IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.boxplot(df.Income)\n",
        "plot.show()\n",
        "sns.boxplot(df.CCAvg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvjNd-OJdyYW",
        "colab_type": "text"
      },
      "source": [
        "_Mortgage_ is highly skewes towards right, hence applying IQR would delete most of important data. Hence we will try to scale it down using Square Root Transformation to check whether the outliers are reduced or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGUDnyUwvYIm",
        "colab_type": "text"
      },
      "source": [
        "We'll handle the _Mortgage_ a little later, first we'll resolve the _Income and CCAVG_ features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaYaQUfVEeSJ",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"report\"></a>\n",
        "Below is a small report which gives us the idea about how the transformations affect our dataset realtionships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmmpznikD9bP",
        "colab_type": "text"
      },
      "source": [
        "Income:\n",
        "\n",
        "1. Initial correlation after removing faulty data: 0.5042\n",
        "2. Correlation after removing outliers using IQR: 0.4889, skewness: 0.8474\n",
        "3. Correlation when Power Transformed: 0.42, skewness: -0.0387\n",
        "4. Correlation when SqRt Transformed: 0.4396, skewness: 0.2238\n",
        "\n",
        "CCAvg:\n",
        "\n",
        "1. Initial correlation after removing faulty data: 0.3693\n",
        "2. Correlation after removing outliers using IQR: 0.3441, skewness: 0.8793\n",
        "3. Correlation when Power Transformed: 0.2859, skewness: 0.0031\n",
        "4. Correlation when SqRt Transformed: 0.0.2911, skewness: -0.0414"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_CF9ETaDqlf",
        "colab_type": "text"
      },
      "source": [
        "Power Transformation is specifically built to reduce the skewness. You might have gone through the small report, you can see how Power Transformation is reducing the skewness of both the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3iqIR5Z4Q98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pt = PowerTransformer(method=\"yeo-johnson\", standardize = False, )\n",
        "pt.fit(df[\"CCAvg\"].values.reshape(-1,1))\n",
        "temp = pt.transform(df[\"CCAvg\"].values.reshape(-1,1))\n",
        "\n",
        "sns.distplot(temp)\n",
        "print(scipy.stats.skew(temp))\n",
        "df[\"CCAvg\"] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYzlRiZ1RGNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pt = PowerTransformer(method=\"box-cox\", standardize = False)\n",
        "pt.fit(df[\"Income\"].values.reshape(-1,1))\n",
        "temp = pt.transform(df[\"Income\"].values.reshape(-1,1))\n",
        "\n",
        "sns.distplot(temp)\n",
        "print(scipy.stats.skew(temp))\n",
        "df[\"Income\"] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRG9cJMiJsng",
        "colab_type": "text"
      },
      "source": [
        "We can see, skewness of <b>Income and CCAvg</b> have been reduced to a large extent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6TMqzaJKhOC",
        "colab_type": "text"
      },
      "source": [
        "Now let's check the updated correlation heatmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tea-HyEyC8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plot.subplots(figsize = (15,10))\n",
        "sns.heatmap(df.corr(), cmap=\"Greys\", annot = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvCCEFSE_kDX",
        "colab_type": "text"
      },
      "source": [
        "I have applied combinations with various other transformations.\n",
        "To check the report, you may visit: https://docs.google.com/document/d/1FI8T-UV2ntleTMvPozZtByskbn93FM_-C8Cvh_ggQok/edit?usp=sharing\n",
        "\n",
        "The below transformation is the best I have got from the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtGf_1E_Q9lD",
        "colab_type": "text"
      },
      "source": [
        "Transformation of _Mortgage_ is done by using Square Root Transformtion. The reason is, this transformation is handling the outliers pretty well.\n",
        "\n",
        "To get in-detail report, you can check the document that I attached above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNItLtmQTWwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mortgage with sqrt\n",
        "df[\"Mortgage\"] = np.sqrt(df[\"Mortgage\"])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9F40MA3MGvZ",
        "colab_type": "text"
      },
      "source": [
        "Boxplotting can help us visualise to check if our Transformation is doing good or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5O-rsucMBi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.boxplot(df.Mortgage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRlUggITvzeW",
        "colab_type": "text"
      },
      "source": [
        "You can even apply binning here. Binning merges all the values of a particular range into a single value. Can say, it is a type of Label Encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVrwacOYjh9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mort_bin = pd.cut(df[\"Mortgage\"], bins=[0, 100, 200, 300, 400, 500, 600, 700], labels=[0, 1, 2, 3, 4, 5, 6], include_lowest=True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukg8b14w4d19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df[\"Mortgage\"] = pd.DataFrame(mort_bin)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwR3E6y_MU-Z",
        "colab_type": "text"
      },
      "source": [
        "One last time, let's again check if our dataset is ready to train or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mH2etGpz0gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkXRPJZUMf3C",
        "colab_type": "text"
      },
      "source": [
        "Our dataset is now far more better than what we initially had."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUT-oU0BM1RH",
        "colab_type": "text"
      },
      "source": [
        "Next step is splitting our dataset for training and testing purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UZl11OnNFXP",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"5\"></a>\n",
        "## 5. Splitting the data into training and test set in the ratio of 70:30 respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqwctHTtNNc5",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to drop our target variable from the dataset and store it in separate variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQNo8n6G7Q3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.drop(\"Personal Loan\", axis=1)\n",
        "y = df[\"Personal Loan\"]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT8UaMg1RWUC",
        "colab_type": "text"
      },
      "source": [
        "If we remeber, the YES:NO ratio is really very very low. We can plot a countplot to recall that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV9E8QYCRuK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T69AGhYeR62N",
        "colab_type": "text"
      },
      "source": [
        "So if we split it randomly, there might be chances that very less amount of \"YES\" would come under training.<br> To avoid this, we use stratified splitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agBgbQD27g-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0, test_size = 0.3)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0AZTbyAUiOm",
        "colab_type": "text"
      },
      "source": [
        "random_states are integer value, which will remember our performance and splitting of our dataset. Inshort, It will preserve our model whenever we split it again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sEckKJYSmeG",
        "colab_type": "text"
      },
      "source": [
        "We have splitted our dataset into training and testing set in 70 : 30 ratio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4fYE1xhSvQu",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"6\"></a>\n",
        "## 6. Using the Logistic Regression model to predict the likelihood of a customer buying personal loans."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KtJLvX18es5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LogisticRegression(max_iter=1000)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJtIsqSdS3bd",
        "colab_type": "text"
      },
      "source": [
        "Now that we have created the object of the Logistic Regression class, next we will fit our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxXQbkBU8_7M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "f5608469-4ebf-43f3-d10a-ea9151a1136c"
      },
      "source": [
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq1KY_RRTCoE",
        "colab_type": "text"
      },
      "source": [
        "We are now done with training our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5D5hyw2TPdA",
        "colab_type": "text"
      },
      "source": [
        "Let's now check how our model performs to the test and train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sISdV5s59Dba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_test = model.predict(X_test)\n",
        "predictions_train = model.predict(X_train)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbv6nKiBTZgV",
        "colab_type": "text"
      },
      "source": [
        "The above 2 variables stores the predicted results. Now we can run various test to check the accuracy, precision, recall and f1-score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZrVEtls7RH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "recall_test_lr = metrics.recall_score(y_test, predictions_test)\n",
        "precision_test_lr = metrics.precision_score(y_test, predictions_test)\n",
        "f1_test_lr = metrics.f1_score(y_test, predictions_test)\n",
        "acc_test_lr = metrics.accuracy_score(y_test, predictions_test)\n",
        "\n",
        "recall_train_lr = metrics.recall_score(y_train, predictions_train)\n",
        "precision_train_lr = metrics.precision_score(y_train, predictions_train)\n",
        "f1_train_lr = metrics.f1_score(y_train, predictions_train)\n",
        "acc_train_lr = metrics.accuracy_score(y_train, predictions_train)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPyDTtawTn5R",
        "colab_type": "text"
      },
      "source": [
        "We can make use of <b>pickle</b> library to save our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD3ebSpNT0jN",
        "colab_type": "text"
      },
      "source": [
        "We can split the dataset multiple times, and check the accuracy score each time. Whenever our accuracy is more than the recorded best score. We will save the new model to the pickle file. \n",
        "<br>\n",
        "So basically we are storing the model having highest accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqtixqXlF5q9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best_test_accuracy = 0\n",
        "# best_train_accuracy = 0\n",
        "# for iteration in range(50):\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.3)\n",
        "\n",
        "#     model = LogisticRegression(max_iter=1000)\n",
        "#     model.fit(X_train, y_train)\n",
        "#     predictions_test = model.predict(X_test)\n",
        "#     predictions_train = model.predict(X_train)\n",
        "#     test_accuracy = metrics.accuracy_score(y_test, predictions_test)\n",
        "#     train_accuracy = metrics.accuracy_score(y_train, predictions_train)\n",
        "\n",
        "#     print(\"Accuracy test: \", test_accuracy)\n",
        "#     print(\"Accuracy train: \", train_accuracy)\n",
        "\n",
        "#     if test_accuracy > best_test_accuracy and train_accuracy > best_train_accuracy:\n",
        "        \n",
        "#         best_test_accuracy = test_accuracy\n",
        "#         best_train_accuracy = train_accuracy\n",
        "#         with open(\"bank_loan_model.pickle\", \"wb\") as file:\n",
        "#             pickle.dump(model, file)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eZZRTU5UQnd",
        "colab_type": "text"
      },
      "source": [
        "Now to access the saved model, we can load that file to our model from now on i.e. we dont't need to train our model again anad again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVRlGMKSKoQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_file = open(\"bank_loan_model.pickle\", \"rb\")\n",
        "# model = pickle.load(model_file)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xGyqpo_V6mU",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"7\"></a>\n",
        "## 7. Metrics calculation for evaluating the model performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnZg7wtuWzw1",
        "colab_type": "text"
      },
      "source": [
        "Report for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9YySS2-DnZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_test = metrics.classification_report(y_test, predictions_test, labels=[0, 1], digits=5)\n",
        "print(\"Report for test data:\\n\\n\",matrix_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eW8y6gkX5Oz",
        "colab_type": "text"
      },
      "source": [
        "Report for train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLhpkC9_FMlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix_train = metrics.classification_report(y_train, predictions_train, labels=[0, 1], digits=5)\n",
        "print(\"Report for train data:\\n\\n\",matrix_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPEM1tCKZWa9",
        "colab_type": "text"
      },
      "source": [
        "Confusion matrix returns a matrix having the correct and incorrect predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_Ct3hjbHHg",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7dHaaLvkOnA",
        "colab_type": "text"
      },
      "source": [
        "TP: True Positive<br>\n",
        "FP: False Positive<br>\n",
        "TN: True Negative<br>\n",
        "FN: False Negative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neD9GvVUlBgP",
        "colab_type": "text"
      },
      "source": [
        "It is better if False positives and False negatives are low."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSHp79tdX-FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf_matrix_test = metrics.confusion_matrix(y_test, predictions_test)\n",
        "conf_matrix_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwzBlMshY7mL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf_matrix_train = metrics.confusion_matrix(y_train, predictions_train)\n",
        "conf_matrix_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uf-IVq_p2ms",
        "colab_type": "text"
      },
      "source": [
        "<b>AUC ROC</b>  are another measure of calculating the efficiency of our model. In simple terms, it is the graph plotted with FPR on x-axis and TPR on y-axis.\n",
        "<hr>\n",
        "TPR: True positive rate<br>\n",
        "FPR: False positive rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTF9OJKttsZK",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"auc-roc\"></a>\n",
        "AUC scores shows how likely our model can predict the values correctly. More the AUC value, better our model will perform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQOOvNK_HP6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions_test)\n",
        "auc_test_lr = metrics.auc(fpr, tpr)\n",
        "auc_test_lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G9wY6oIomZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot.plot(fpr, tpr)\n",
        "plot.xlabel(\"False Positive Rate\")\n",
        "plot.ylabel(\"True Positive Rate\")\n",
        "plot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SOCD5T7yJpg",
        "colab_type": "text"
      },
      "source": [
        "Our graph seems pretty well "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMBBh4tYaL6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(y_train, predictions_train)\n",
        "auc_train_lr = metrics.auc(fpr, tpr)\n",
        "auc_train_lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLnZElNDpo2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot.plot(fpr, tpr)\n",
        "plot.xlabel(\"False Positive Rate\")\n",
        "plot.ylabel(\"True Positive Rate\")\n",
        "plot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLfd_FDnyQTd",
        "colab_type": "text"
      },
      "source": [
        "Even on Train data the performance is balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BTg1vqbuoRm",
        "colab_type": "text"
      },
      "source": [
        "This ends our evaluation on Logistic Regression. Now we'll check how our dataset performs with other classification algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHGo_I3tu0Vu",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"8\"></a>\n",
        "## 8. Comparison with other Classification algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCKruyF4v7ob",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"dt\"></a>\n",
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGxjPxSLFuZR",
        "colab_type": "text"
      },
      "source": [
        "A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node holds a class label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1rBkRzjdADk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dt = DecisionTreeClassifier(max_depth=10, criterion=\"entropy\")\n",
        "model_dt.fit(X_train, y_train)\n",
        "predictions_test_dt = model_dt.predict(X_test)\n",
        "predictions_train_dt = model_dt.predict(X_train)\n",
        "model_dt_test_report = metrics.classification_report(y_test, predictions_test_dt, digits=5)\n",
        "print(\"Report for Test Data:\\n\",model_dt_test_report)\n",
        "model_dt_train_report = metrics.classification_report(y_train, predictions_train_dt, digits=5)\n",
        "print(\"Report for Train Data:\\n\",model_dt_train_report)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions_test_dt)\n",
        "auc_test_dt = metrics.auc(fpr, tpr)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_train, predictions_train_dt)\n",
        "auc_train_dt = metrics.auc(fpr, tpr)\n",
        "\n",
        "#Confusion Matrices\n",
        "print(\"\\nConfusion matrix for Testing Data:\\n\",metrics.confusion_matrix(y_test, predictions_test_dt))\n",
        "print(\"\\nConfusion matrix for Training Data:\\n\",metrics.confusion_matrix(y_train, predictions_train_dt))\n",
        "\n",
        "#individual scores\n",
        "recall_test_dt = metrics.recall_score(y_test, predictions_test_dt)\n",
        "precision_test_dt = metrics.precision_score(y_test, predictions_test_dt)\n",
        "f1_test_dt = metrics.f1_score(y_test, predictions_test_dt)\n",
        "acc_test_dt = metrics.accuracy_score(y_test, predictions_test_dt)\n",
        "\n",
        "recall_train_dt = metrics.recall_score(y_train, predictions_train_dt)\n",
        "precision_train_dt = metrics.precision_score(y_train, predictions_train_dt)\n",
        "f1_train_dt = metrics.f1_score(y_train, predictions_train_dt)\n",
        "acc_train_dt = metrics.accuracy_score(y_train, predictions_train_dt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCVDU6v3wSI_",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"rf\"></a>\n",
        "### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC2co5LSGCoY",
        "colab_type": "text"
      },
      "source": [
        "Random forest is a method, where a dataset is split into multiple sets and each is then trained using Decision trees. At the end, the output having maximum occurence is considered. Basically, as many trees are iterating at a time, hence the name Random forest. It is based on maximum vote system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMjbwCXRf0AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_rf = RandomForestClassifier(n_estimators=10, max_depth=8, criterion=\"entropy\")\n",
        "model_rf.fit(X_train, y_train)\n",
        "predictions_test_rf = model_rf.predict(X_test)\n",
        "predictions_train_rf = model_rf.predict(X_train)\n",
        "report_test_rf = metrics.classification_report(y_test, predictions_test_rf, digits=5)\n",
        "print(\"Report for Test Data:\\n\",report_test_rf)\n",
        "report_train_rf = metrics.classification_report(y_train, predictions_train_rf, digits=5)\n",
        "print(\"Report for Train Data:\\n\",report_train_rf)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions_test_rf)\n",
        "auc_test_rf = metrics.auc(fpr, tpr)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_train, predictions_train_rf)\n",
        "auc_train_rf = metrics.auc(fpr, tpr)\n",
        "\n",
        "#Confusion Matrices\n",
        "print(\"\\nConfusion matrix for Testing Data:\\n\",metrics.confusion_matrix(y_test, predictions_test_rf))\n",
        "print(\"\\nConfusion matrix for Traing Data:\\n\",metrics.confusion_matrix(y_train, predictions_train_rf))\n",
        "\n",
        "#individual scores\n",
        "recall_test_rf = metrics.recall_score(y_test, predictions_test_rf)\n",
        "precision_test_rf = metrics.precision_score(y_test, predictions_test_rf)\n",
        "f1_test_rf = metrics.f1_score(y_test, predictions_test_rf)\n",
        "acc_test_rf = metrics.accuracy_score(y_test, predictions_test_rf)\n",
        "\n",
        "recall_train_rf = metrics.recall_score(y_train, predictions_train_rf)\n",
        "precision_train_rf = metrics.precision_score(y_train, predictions_train_rf)\n",
        "f1_train_rf = metrics.f1_score(y_train, predictions_train_rf)\n",
        "acc_train_rf = metrics.accuracy_score(y_train, predictions_train_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "449gRUhWwtlC",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"nb\"></a>\n",
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbM4y18tG-TR",
        "colab_type": "text"
      },
      "source": [
        "Naive Bayes classifiers are a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0Xudy3ohL2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_nb = GaussianNB()\n",
        "model_nb.fit(X_train, y_train)\n",
        "predictions_test_nb = model_nb.predict(X_test)\n",
        "predictions_train_nb = model_nb.predict(X_train)\n",
        "report_test_nb = metrics.classification_report(y_test, predictions_test_nb, digits=5)\n",
        "print(\"Report for Test Data:\\n\",report_test_nb)\n",
        "report_train_nb = metrics.classification_report(y_train, predictions_train_nb, digits=5)\n",
        "print(\"Report for Train Data:\\n\",report_train_nb)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions_test_nb)\n",
        "auc_test_nb = metrics.auc(fpr, tpr)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_train, predictions_train_nb)\n",
        "auc_train_nb = metrics.auc(fpr, tpr)\n",
        "\n",
        "#Confusion Matrices\n",
        "print(\"\\nConfusion matrix for Testing Data:\\n\",metrics.confusion_matrix(y_test, predictions_test_nb))\n",
        "print(\"\\nConfusion matrix for Traing Data:\\n\",metrics.confusion_matrix(y_train, predictions_train_nb))\n",
        "\n",
        "#individual scores\n",
        "recall_test_nb = metrics.recall_score(y_test, predictions_test_nb)\n",
        "precision_test_nb = metrics.precision_score(y_test, predictions_test_nb)\n",
        "f1_test_nb = metrics.f1_score(y_test, predictions_test_nb)\n",
        "acc_test_nb = metrics.accuracy_score(y_test, predictions_test_nb)\n",
        "\n",
        "recall_train_nb = metrics.recall_score(y_train, predictions_train_nb)\n",
        "precision_train_nb = metrics.precision_score(y_train, predictions_train_nb)\n",
        "f1_train_nb = metrics.f1_score(y_train, predictions_train_nb)\n",
        "acc_train_nb = metrics.accuracy_score(y_train, predictions_train_nb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cndf3r7hw-H5",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"knn\"></a>\n",
        "### KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eecnMOObHHhG",
        "colab_type": "text"
      },
      "source": [
        "KNN is an acronym for K Nearest Neighbours. This algorithm divides the scattered into a specific class based on their properties. At the testing, it checks the properties of the neighbouring points and based on the probability of each class it will classify the point into the predicted class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mpSLUzcmKDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "model_knn.fit(X_train, y_train)\n",
        "predictions_test_knn = model_knn.predict(X_test)\n",
        "predictions_train_knn = model_knn.predict(X_train)\n",
        "knn_test_report = metrics.classification_report(y_test, predictions_test_knn, digits = 5)\n",
        "print(\"Report for Test Data:\\n\",knn_test_report)\n",
        "knn_train_report = metrics.classification_report(y_train, predictions_train_knn, digits = 5)\n",
        "print(\"Report for Train Data:\\n\",knn_train_report)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions_test_knn)\n",
        "auc_test_knn = metrics.auc(fpr, tpr)\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_train, predictions_train_knn)\n",
        "auc_train_knn = metrics.auc(fpr, tpr)\n",
        "\n",
        "#Confusion Matrices\n",
        "print(\"\\nConfusion matrix for Testing Data:\\n\",metrics.confusion_matrix(y_test, predictions_test_knn))\n",
        "print(\"\\nConfusion matrix for Traing Data:\\n\",metrics.confusion_matrix(y_train, predictions_train_knn))\n",
        "\n",
        "#individual scores\n",
        "recall_test_knn = metrics.recall_score(y_test, predictions_test_knn)\n",
        "precision_test_knn = metrics.precision_score(y_test, predictions_test_knn)\n",
        "f1_test_knn = metrics.f1_score(y_test, predictions_test_knn)\n",
        "acc_test_knn = metrics.accuracy_score(y_test, predictions_test_knn)\n",
        "\n",
        "recall_train_knn = metrics.recall_score(y_train, predictions_train_knn)\n",
        "precision_train_knn = metrics.precision_score(y_train, predictions_train_knn)\n",
        "f1_train_knn = metrics.f1_score(y_train, predictions_train_knn)\n",
        "acc_train_knn = metrics.accuracy_score(y_train, predictions_train_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCgp4QXfQJR",
        "colab_type": "text"
      },
      "source": [
        "### Report "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMcArfRUHyLe",
        "colab_type": "text"
      },
      "source": [
        "Now as we have tested our dataset with various other algorithms, let's summarize them now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qUOvIlbq-n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report_test = pd.DataFrame(\n",
        "    {\n",
        "        \"Models\":[\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"Naive Bayes\", \"KNN\"],\n",
        "        \"Accuracy\":[acc_test_lr, acc_test_dt, acc_test_rf, acc_test_nb, acc_test_knn],\n",
        "        \"Recall\":[recall_test_lr, recall_test_dt, recall_test_rf, recall_test_nb, recall_test_knn],\n",
        "        \"Precision\":[precision_test_lr, precision_test_dt, precision_test_rf, precision_test_nb, precision_test_knn],\n",
        "        \"F1-score\":[f1_test_lr, f1_test_dt, f1_test_rf, f1_test_nb, f1_test_knn],\n",
        "        \"AUC-ROC score\":[auc_test_lr, auc_test_dt, auc_test_rf, auc_test_nb, auc_test_knn] \n",
        "    }\n",
        ")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X9E62xN84Yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report_train = pd.DataFrame(\n",
        "    {\n",
        "        \"Models\":[\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"Naive Bayes\", \"KNN\"],\n",
        "        \"Accuracy\":[acc_train_lr, acc_train_dt, acc_train_rf, acc_train_nb, acc_train_knn],\n",
        "        \"Recall\":[recall_train_lr, recall_train_dt, recall_train_rf, recall_train_nb, recall_train_knn],\n",
        "        \"Precision\":[precision_train_lr, precision_train_dt, precision_train_rf, precision_train_nb, precision_train_knn],\n",
        "        \"F1-score\":[f1_train_lr, f1_train_dt, f1_train_rf, f1_train_nb, f1_train_knn],\n",
        "        \"AUC-ROC score\":[auc_train_lr, auc_train_dt, auc_train_rf, auc_train_nb, auc_train_knn] \n",
        "    }\n",
        ")"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toDsRmOlsuJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Test Data report:\\n\\n\")\n",
        "report_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_iLl7Eb9XOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Train Data report:\\n\\n\")\n",
        "report_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0wY6Ko6IA6N",
        "colab_type": "text"
      },
      "source": [
        "Hence, observing the above reports, we can conclude that our dataset works better with **Random forest** and **Decision tree** algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zWNwUW6Iw4A",
        "colab_type": "text"
      },
      "source": [
        "## Business understanding of our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAoOi81NVA8p",
        "colab_type": "text"
      },
      "source": [
        "<h1>Table of Contents</h1>\n",
        "\n",
        "- [Import](#import)\n",
        "- [Cleaning](#cleaning)\n",
        "- [EDA](#eda)\n",
        "- [Transformations](#transformations)\n",
        "- [Splitting](#splitting)\n",
        "- [Training](#training)\n",
        "- [Metrics Evaluation](#metrics-evaluation)\n",
        "- [Comparisons](#comparisons)\n",
        "  * [Decision Tree Classifier](#decision-tree-classifier)\n",
        "  * [Random Forest Classifier](#random-forest-classifier)\n",
        "  * [Naive Bayes](#naive-bayes)\n",
        "  * [K Nearest Neighbours](#k-nearest-neighbours)\n",
        "- [Conclusion](#conclusion)\n",
        "\n",
        "\n",
        "\n",
        "[<h1>Import</h1>](#1)<a name=\"import\"></a>\n",
        "\n",
        "We have imported all the necessary required models for our project.<br>\n",
        "\n",
        "[<h1>Cleaning</h1>](#2) <a name=\"cleaning\"></a>\n",
        "\n",
        "Looking into the raw dataset, we observed that there were some faulty data (i.e. values were below 0) in _Experience_ columns. Hence, as the data was incorrect, it is better to remove the entire rows from our dataset.<br>\n",
        "\n",
        "_ID_ being different for each customer, it was better to drop that column also. Also, when we tested the correlations of feature variables with our Target variable, it was observed that _Age and Experience_ was not so related with Target variable. Aslo noticed one thing, _ZIP Code_ was also not of much use. So, in the end, we dropped 4 columns, which are\n",
        "- _ID_\n",
        "- _Age_\n",
        "- _Experience_\n",
        "- _ZIP Code_\n",
        "\n",
        "[<h1>EDA</h1>](#3)<a name=\"eda\"></a>\n",
        "\n",
        "We did the Exploratory Data Analysis([EDA](#abcd)) to check how our data is distributed. Initially we checked the number of unique elements in each column. We observed, that _Income, CCAvg and Mortgage_ had a bunch of unique elements. Hence the chances of having the outliers in more in this case. It was also seen using [countplot](#countplot) that very few customers took the _Personal Loan_.\n",
        "One more thing to notice was that, people with **0** _Mortgage_ were very high, which might highly skew our distribution to the right. Plotting various graphs helped us understand their distribution and their relationship with the target variable. We then plotted the correlation heatmap to see the relationships ofthe variables with each other. To check the presence of outliers, we plotted [**boxplot**](#boxplot) which marks the outliers of the column. All three variables(_Income, CCAvg and Mortgage_) were having outliers. Plotting [**distplot**](#distplot) helped us understand the skewness of the graph. The distributions were skewed to the right. It was seen, with increase in _Income_, there was increase in probabibility of the custmer buying the _Personal Loan_. [**Pairplotting**](#pairplot) displayed the graphs of variable against each other, all at once. <br>\n",
        "\n",
        "[<h1>Transformations</h1>](#4)<a name=\"transformations\"></a>\n",
        "\n",
        "Now, we tried to remove the [outliers](#outlier) using [IQR method](#IQR).\n",
        "\n",
        "> Outliers are values which are either very low or very large as compared to other values of that column. \n",
        "\n",
        "Many outliers were removed after applying the IQR method. [check here](#outlier-removed)\n",
        "<br>\n",
        "As the _Mortgage_ was highly skewed(even high number of \"0\"), applying IQR would probably treat many important values as the outlier. Hence, it is not recommended to use IQR in this case.\n",
        "\n",
        "After removing the outliers, the skewness was not that reduced. We need too appy certain transformations to make our distribution Normal. I performed several test and formulated a report where we can see how features vary with different transformation.\n",
        "\n",
        "---\n",
        "\n",
        "1. Initial correlation after removing faulty data: 0.5042\n",
        "2. Correlation after removing outliers using IQR: 0.4889, skewness: 0.8474\n",
        "3. Correlation when Power Transformed: 0.42, skewness: -0.0387\n",
        "4. Correlation when SqRt Transformed: 0.4396, skewness: 0.2238\n",
        "\n",
        "CCAvg:\n",
        "\n",
        "1. Initial correlation after removing faulty data: 0.3693\n",
        "2. Correlation after removing outliers using IQR: 0.3441, skewness: 0.8793\n",
        "3. Correlation when Power Transformed: 0.2859, skewness: 0.0031\n",
        "4. Correlation when SqRt Transformed: 0.0.2911, skewness: -0.0414\n",
        "\n",
        "---\n",
        "We can see, Power Transformation is reducing the skewness to a great extent but we have too compromise on the correlation. Similarly, Square root transformation is also reducing the skewness(not much as that of Power Transformation) but is maintaining the correlation quite a bit more than the Power Tansformation. As _Mortgage_ is highly skewed, we can apply either binning or square root transformation. \n",
        "> Binning is the process where we label or cap the values in a particular range to a single value i.e. let's say, all values between 0 to 100 will be treated as 0 only. Similar to Label encoding.\n",
        "\n",
        "I tried different combinations for transformations on different features and observed, _Income and CCAvg_ were performing good with Power transformation while Square root transformation went good with _Mortgage_ as it removed all the outliers.\n",
        "To get an idea of the analysis, you can visit this [link](https://docs.google.com/document/d/1FI8T-UV2ntleTMvPozZtByskbn93FM_-C8Cvh_ggQok/edit?usp=sharing) where I attached the results.\n",
        "To check if the dataset is clean, we again plotted the boxplots for each of these three features and it can be seen that there are NO OUTLIERS and OUR DISTRIBUTION IS ALSO NORMAL NOW.\n",
        "<br>\n",
        "\n",
        "[<h1>Splitting</h1>](#5) <a name=\"splitting\"></a>\n",
        "\n",
        "We then separated our target variable from the dataset and stored it seperately.\n",
        "\n",
        "If you remember, there were very less people who opted for _Personal Loan_. So, if we split it randomly, there might be chances that our model will get trained oonly for \"NO\" values. To avoid this, we used stratified splitting. We used the sklearn's inbuilt test_train_method to split the data in 70:30 ratio. \n",
        "- 70% for training\n",
        "- 30% for testing.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "[<h1>Training</h1>](#6)<a name=\"training\"></a>\n",
        "\n",
        "Initially we tried to train our model using Logistic Regression. \n",
        "> Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. \n",
        "\n",
        "Once the training/fitting was done, we tested our model by passing the test set to check the accuracy and metrics evaluation.\n",
        "\n",
        "We can even use pickle library to save our model and later directly load the model without training it again. \n",
        "\n",
        "[<h1>Metrics Evaluation</h1>](#7) <a name=\"metrics-evaluation\"></a>\n",
        "\n",
        "Before diving into the results, let's first see what these values mean\n",
        "\n",
        "\n",
        ">True Positive (TP) : <br>\n",
        "The predicted value matches the actual value\n",
        "The actual value was positive and the model predicted a positive value.\n",
        "\n",
        ">True Negative (TN):<br>\n",
        "The predicted value matches the actual value\n",
        "The actual value was negative and the model predicted a negative value.\n",
        "\n",
        ">False Positive (FP)  Type 1 error:<br>\n",
        "The predicted value was falsely predicted\n",
        "The actual value was negative but the model predicted a positive value\n",
        "Also known as the Type 1 error.\n",
        "\n",
        ">False Negative (FN)  Type 2 error:<br>\n",
        "The predicted value was falsely predicted\n",
        "The actual value was positive but the model predicted a negative value\n",
        "Also known as the Type 2 error.\n",
        "\n",
        "---\n",
        "\n",
        "> Precision tells us how many of the correctly predicted cases actually turned out to be positive. Precision is a useful metric in cases where False Positive is a higher concern than False Negatives.\n",
        "\n",
        "> Recall tells us how many of the actual positive cases we were able to predict correctly with our model. Recall is a useful metric in cases where False Negative trumps False Positive.\n",
        "\n",
        "> F1-score is a harmonic mean of Precision and Recall\n",
        "\n",
        "> AUC ROC are another measure of calculating the efficiency of our model. In simple terms, it is the graph plotted with FPR on x-axis and TPR on y-axis.\n",
        "\n",
        "---\n",
        "Now let's see what results we got.\n",
        "\n",
        "The results of testing are:\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.97531   | 0.98982 | 0.98251  | 1277    |\n",
        "| 1            | 0.84524   | 0.68932 | 0.75936  | 103     |\n",
        "| accuracy     |           |         | 0.96739  | 1380    |\n",
        "| macro avg    | 0.91027   | 0.83957 | 0.80793  | 1380    |\n",
        "| weighted avg | 0.96560   | 0.96739 | 0.98586  | 1380    |\n",
        "\n",
        "The results for training are:\n",
        "\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.96815   | 0.99026 | 0.97908  | 2978    |\n",
        "| 1            | 0.83237   | 0.59751 | 0.69565  | 241     |\n",
        "| accuracy     |           |         | 0.96086  | 3219    |\n",
        "| macro avg    | 0.90026   | 0.79389 | 0.83737  | 3219    |\n",
        "| weighted avg | 0.95799   | 0.96086 | 0.95786  | 3219    |\n",
        "\n",
        "Confusion matrix for testing data is:\n",
        "\n",
        "\n",
        "| 1264 | 13 |\n",
        "|-|-|\n",
        "| 32   | 71 |\n",
        "\n",
        "Auc scores:\n",
        "\n",
        "For testing: 0.8395701393587823\n",
        "\n",
        "For training: 0.793886147098083\n",
        "\n",
        "We can the plot the [AUC-ROC curves](#auc-roc) for both the data. They seem pretty good.\n",
        "\n",
        "[<h1>Comparisons</h1>](#8)<a name=\"comparisons\"></a>\n",
        "\n",
        "We even trained our model using various other algorithmns, the results are displayed below\n",
        "\n",
        "---\n",
        "\n",
        "<h3> Decision Tree Classifier </h3> <a name=\"decision-tree-classifier\"></a>\n",
        "> A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node holds a class label.\n",
        "\n",
        "Results for testing:\n",
        "\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.98671   | 0.98825 | 0.98748  | 1277    |\n",
        "| 1            | 0.85149   | 0.83495 | 0.84314  | 103     |\n",
        "| accuracy     |           |         | 0.97681  | 1380    |\n",
        "| macro avg    | 0.91910   | 0.91160 | 0.91531  | 1380    |\n",
        "| weighted avg | 0.97662   | 0.97681 | 0.97671  | 1380    |\n",
        "\n",
        "Results for training:\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.99632   | 0.99966 | 0.99799  | 2978    |\n",
        "| 1            | 0.99567   | 0.95436 | 0.97458  | 241     |\n",
        "| accuracy     |           |         | 0.99627  | 3219    |\n",
        "| macro avg    | 0.99599   | 0.97701 | 0.98628  | 3219    |\n",
        "| weighted avg | 0.99627   | 0.99627 | 0.99624  | 3219    |\n",
        "\n",
        "\n",
        "Confusion matrix for Testing Data:\n",
        "\n",
        " \n",
        "| 1262 | 15 |\n",
        "|-|-|\n",
        "| 17   | 86 |\n",
        "\n",
        "---\n",
        "\n",
        "<h3> Random Forest Classifier</h3> <a name=\"random-forest-classifier\"></a>\n",
        "\n",
        "> Random forest is a method, where a dataset is split into multiple sets and each is then trained using Decision trees. At the end, the output having maximum occurence is considered. Basically, as many trees are iterating at a time, hence the name Random forest. It is based on maximum vote system.\n",
        "\n",
        "Results for testing\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.97989   | 0.99217 | 0.98599  | 1277    |\n",
        "| 1            | 0.88506   | 0.74757 | 0.81053  | 103     |\n",
        "| accuracy     |           |         | 0.97391  | 1380    |\n",
        "| macro avg    | 0.93247   | 0.86987 | 0.89826  | 1380    |\n",
        "| weighted avg | 0.97281   | 0.97391 | 0.97290  | 1380    |\n",
        "\n",
        "\n",
        "\n",
        "Results for training\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.99331   | 0.99765 | 0.99548  | 2978    |\n",
        "| 1            | 0.96930   | 0.91701 | 0.94243  | 241     |\n",
        "| accuracy     |           |         | 0.99161  | 3219    |\n",
        "| macro avg    | 0.98131   | 0.95733 | 0.96895  | 3219    |\n",
        "| weighted avg | 0.99152   | 0.99161 | 0.99151  | 3219    |\n",
        "\n",
        "\n",
        "Confusion matrix for Testing Data:\n",
        "\n",
        " | 1267 | 10 |\n",
        "|-|-|\n",
        "| 26   | 77 |\n",
        "\n",
        "---\n",
        "\n",
        "<h3> Naive Bayes </h3> <a name=\"naive-bayes\"></a>\n",
        "\n",
        "> Naive Bayes classifiers are a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other\n",
        "\n",
        "\n",
        "Results for testing\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.95863   | 0.96163 | 0.96013  | 1277    |\n",
        "| 1            | 0.50505   | 0.48544 | 0.49505  | 103     |\n",
        "| accuracy     |           |         | 0.92609  | 1380    |\n",
        "| macro avg    | 0.73184   | 0.72353 | 0.72759  | 1380    |\n",
        "| weighted avg | 0.92477   | 0.92609 | 0.92541  | 1380    |\n",
        "\n",
        "\n",
        "\n",
        "Results for training\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.95697   | 0.97079 | 0.96383  | 2978    |\n",
        "| 1            | 0.56061   | 0.46058 | 0.50569  | 241     |\n",
        "| accuracy     |           |         | 0.93259  | 3219    |\n",
        "| macro avg    | 0.75879   | 0.71568 | 0.73476  | 3219    |\n",
        "| weighted avg | 0.92729   | 0.93259 | 0.92953  | 3219    |\n",
        "\n",
        "Confusion matrix for Testing Data:\n",
        "\n",
        "  | 1228 | 49 |\n",
        "|-|-|\n",
        "| 53   | 50 |\n",
        "\n",
        "---\n",
        "\n",
        "<h3> K Nearest Neighbours </h3><a name=\"k-nearest-neighbours\"></a>\n",
        "\n",
        "> KNN is an acronym for K Nearest Neighbours. This algorithm divides the scattered into a specific class based on their properties. At the testing, it checks the properties of the neighbouring points and based on the probability of each class it will classify the point into the predicted class.\n",
        "\n",
        "\n",
        "Results for testing\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.97097   | 0.99530 | 0.98299  | 1277    |\n",
        "| 1            | 0.91549   | 0.63107 | 0.74713  | 103     |\n",
        "| accuracy     |           |         | 0.96812  | 1380    |\n",
        "| macro avg    | 0.94323   | 0.81318 | 0.86506  | 1380    |\n",
        "| weighted avg | 0.96683   | 0.96812 | 0.96538  | 1380    |\n",
        "\n",
        "\n",
        "\n",
        "Results for training\n",
        "\n",
        "|              | precision | recall  | f1-score | support |\n",
        "|--------------|-----------|---------|----------|---------|\n",
        "| 0            | 0.97857   | 0.99664 | 0.98752  | 2978    |\n",
        "| 1            | 0.94624   | 0.73029 | 0.82436  | 241     |\n",
        "| accuracy     |           |         | 0.97670  | 3219    |\n",
        "| macro avg    | 0.96240   | 0.86347 | 0.90594  | 3219    |\n",
        "| weighted avg | 0.97615   | 0.97670 | 0.97531  | 3219    |\n",
        "\n",
        "\n",
        "\n",
        "Confusion matrix for Testing Data:\n",
        "\n",
        "\n",
        "   | 1271 | 6 |\n",
        "|-|-|\n",
        "| 38   | 65 |\n",
        "\n",
        "<h1> Final Report of comparison\n",
        "\n",
        "For Test data:\n",
        "\n",
        "\n",
        "**Models**|**Accuracy**|**Recall**|**Precision**|**F1-score**|**AUC-ROC score**\n",
        ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
        "\tLogistic Regression|\t0.967391|\t0.689320|\t0.845238|\t0.759358|\t0.839570\n",
        "\tDecision Tree|\t0.976812|\t0.834951|\t0.851485|\t0.843137|\t0.911603\n",
        "\tRandom Forest|\t0.973913|\t0.747573|\t0.885057|\t0.810526|\t0.869871\n",
        "\tNaive Bayes|\t0.926087|\t0.485437|\t0.505051|\t0.495050|\t0.723533\n",
        "\tKNN|\t0.968116|\t0.631068|\t0.915493|\t0.747126|\t0.813185\n",
        "\n",
        "For Train data:\n",
        "\n",
        "**Models**|**Accuracy**|**Recall**|**Precision**|**F1-score**|**AUC-ROC score**\n",
        ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
        "Logistic Regression|0.960857|0.59751|0.83237|0.695652|0.793886\n",
        "Decision Tree|0.996272|0.954357|0.995671|0.974576|0.977011\n",
        "Random Forest|0.991612|0.917012|0.969298|0.942431|0.957331\n",
        "Naive Bayes|0.932588|0.460581|0.560606|0.505695|0.715683\n",
        "KNN|0.976701|0.730290|0.946237|0.824356|0.863466\n",
        "\n",
        "**<h2>Conclusion:</h2>** <a name=\"conclusion\"></a>\n",
        "By looking at our reports, we can conclude that this dataset is being handled better by **Decision Tree Classifier** and **Random Forest Classifier**.\n",
        "If we order them in priority order, it would be:\n",
        "1. [Decision Tree Classifier](#dt)\n",
        "2. [Random Forest Classifier](#rf)\n",
        "3. [Logistic Regression](#6)\n",
        "4. [K Nearest Neighbours](#knn)\n",
        "5. [Naive Bayes](#nb)"
      ]
    }
  ]
}